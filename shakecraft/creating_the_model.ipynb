{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab all the text files\n",
    "\n",
    "the works of shakespeare and lovecraft can be found here:\n",
    "\n",
    "[The complete works of Shakespeare](https://www.thecompleteworksofshakespeare.com/)\n",
    "\n",
    "[Electronic Texts of H.P. Lovecraft’s Works](https://www.hplovecraft.com/writings/texts/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_txt = glob.glob(os.path.join(os.getcwd(),\"stories/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/henning/work/playground_2/shakecraft/stories/hp_the_colour_out_of_space.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_the_gentlemen_of_verona.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_shadow_over_innsmouth.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_what_the_moon_brings.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_case_of_charles_dexter_ward.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_battle_that_ended_the_century.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_silver_key.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_julius_casesar.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_horror_in_the_burying_ground.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_horror_in_the_museum.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_secret_cave.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_the_sonnets.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_dunwich_horror.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_memory.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_hamlet.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_ashes.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_night_ocean.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_history_of_the_necronomicon.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_terrible_old_man.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_very_old_folk.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_winters_tale.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_pickmans_model.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_diary_of_alonzo_typer.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_dreams_in_the_witch_house.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_antony_and_cleopatra.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_last_test.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_two_black_bottles.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_othello.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_azathoth.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_whisperer_in_darkness.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_cats_of_ulthar.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_as_you_like_it.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_thing_on_the_doorstep.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_beast_in_the_cave.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_slaying_of_the_monster.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_medusas_coil.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_horror_at_red_hook.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_call_of_cthulhu.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_ex_oblivione.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_the_taming_of_the_shrew.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_tempest.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_doom_that_came_to_sarnath.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_tree.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_ghost_eater.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_hound.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_winged_death.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_pericles_price_of_tyre.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_statement_of_randolph_carter.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_disinterment.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_electric_executioner.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_the_loved_dead.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/hp_herbert_west_reanimator.txt',\n",
       " '/home/henning/work/playground_2/shakecraft/stories/shk_macbeth.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join all the texts together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_txt = []\n",
    "\n",
    "for tmp_txt in all_txt:\n",
    "    with open(tmp_txt,'r') as f:\n",
    "        big_txt.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_txt = \" \".join(big_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(complete_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make it so that each character has a corresponding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char:idx for idx,char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_txt = np.array([char_to_idx[c] for c in complete_txt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab some text to get a feel of how long each sequence should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West of Arkham the hills rise wild, and there are valleys with deep woods that no axe has ever cut. There are dark narrow glens where the trees slope fantastically, and where thin brooklets trickle without ever having caught the glint of sunlight. On the gentler slopes there are farms, ancient and rocky, with squat, moss-coated cottages brooding eternally over old New England secrets in the lee of great ledges; but these are all vacant now, the wide chimneys crumbling and the shingled sides bulg\n"
     ]
    }
   ],
   "source": [
    "print(complete_txt[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_part = '''\n",
    "West of Arkham the hills rise wild, and there are valleys with deep woods \n",
    "that no axe has ever cut. There are dark narrow glens where the trees slope \n",
    "fantastically, and where thin brooklets trickle without ever having caught \n",
    "the glint of sunlight.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(some_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 250\n",
    "total_num_seq = len(complete_txt)//(seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13538"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a dataset with an input and target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len+1,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1] # woof woo\n",
    "    target_txt = seq[1:] # oof woof\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab a sequence from the dataset and see how the input and target looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48 58 72 73  2 68 59  2 26 71 64 61 54 66  2 73 61 58  2 61 62 65 65 72\n",
      "  2 71 62 72 58  2 76 62 65 57 10  2 54 67 57  2 73 61 58 71 58  2 54 71\n",
      " 58  2 75 54 65 65 58 78 72  2 76 62 73 61  2 57 58 58 69  2 76 68 68 57\n",
      " 72  2 73 61 54 73  2 67 68  2 54 77 58  2 61 54 72  2 58 75 58 71  2 56\n",
      " 74 73 12  2 45 61 58 71 58  2 54 71 58  2 57 54 71 64  2 67 54 71 71 68\n",
      " 76  2 60 65 58 67 72  2 76 61 58 71 58  2 73 61 58  2 73 71 58 58 72  2\n",
      " 72 65 68 69 58  2 59 54 67 73 54 72 73 62 56 54 65 65 78 10  2 54 67 57\n",
      "  2 76 61 58 71 58  2 73 61 62 67  2 55 71 68 68 64 65 58 73 72  2 73 71\n",
      " 62 56 64 65 58  2 76 62 73 61 68 74 73  2 58 75 58 71  2 61 54 75 62 67\n",
      " 60  2 56 54 74 60 61 73  2 73 61 58  2 60 65 62 67 73  2 68 59  2 72 74\n",
      " 67 65 62 60 61 73 12  2 40 67]\n",
      "West of Arkham the hills rise wild, and there are valleys with deep woods that no axe has ever cut. There are dark narrow glens where the trees slope fantastically, and where thin brooklets trickle without ever having caught the glint of sunlight. On\n",
      "\n",
      "\n",
      "[58 72 73  2 68 59  2 26 71 64 61 54 66  2 73 61 58  2 61 62 65 65 72  2\n",
      " 71 62 72 58  2 76 62 65 57 10  2 54 67 57  2 73 61 58 71 58  2 54 71 58\n",
      "  2 75 54 65 65 58 78 72  2 76 62 73 61  2 57 58 58 69  2 76 68 68 57 72\n",
      "  2 73 61 54 73  2 67 68  2 54 77 58  2 61 54 72  2 58 75 58 71  2 56 74\n",
      " 73 12  2 45 61 58 71 58  2 54 71 58  2 57 54 71 64  2 67 54 71 71 68 76\n",
      "  2 60 65 58 67 72  2 76 61 58 71 58  2 73 61 58  2 73 71 58 58 72  2 72\n",
      " 65 68 69 58  2 59 54 67 73 54 72 73 62 56 54 65 65 78 10  2 54 67 57  2\n",
      " 76 61 58 71 58  2 73 61 62 67  2 55 71 68 68 64 65 58 73 72  2 73 71 62\n",
      " 56 64 65 58  2 76 62 73 61 68 74 73  2 58 75 58 71  2 61 54 75 62 67 60\n",
      "  2 56 54 74 60 61 73  2 73 61 58  2 60 65 62 67 73  2 68 59  2 72 74 67\n",
      " 65 62 60 61 73 12  2 40 67  2]\n",
      "est of Arkham the hills rise wild, and there are valleys with deep woods that no axe has ever cut. There are dark narrow glens where the trees slope fantastically, and where thin brooklets trickle without ever having caught the glint of sunlight. On \n"
     ]
    }
   ],
   "source": [
    "for input_txt, target_txt in  dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(idx_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    print(''.join(idx_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "buffer_size = 100000\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(256, 250), dtype=tf.int64, name=None), TensorSpec(shape=(256, 250), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the model parameters\n",
    "\n",
    "load the necessary things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 128\n",
    "# embed_dim = 64\n",
    "# rnn_neurons = 1024\n",
    "rnn_neurons = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 107\n",
      "embed_dim = 128\n",
      "rnn_neurons = 512\n",
      "batch_size = 256\n",
      "char_to_idx = {'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '#': 4, '$': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, ',': 10, '-': 11, '.': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, '[': 52, ']': 53, 'a': 54, 'b': 55, 'c': 56, 'd': 57, 'e': 58, 'f': 59, 'g': 60, 'h': 61, 'i': 62, 'j': 63, 'k': 64, 'l': 65, 'm': 66, 'n': 67, 'o': 68, 'p': 69, 'q': 70, 'r': 71, 's': 72, 't': 73, 'u': 74, 'v': 75, 'w': 76, 'x': 77, 'y': 78, 'z': 79, '°': 80, '½': 81, 'Å': 82, 'Æ': 83, '×': 84, 'á': 85, 'ä': 86, 'æ': 87, 'è': 88, 'é': 89, 'ë': 90, 'ó': 91, 'ö': 92, 'ü': 93, 'Ο': 94, 'α': 95, 'δ': 96, 'ἶ': 97, '–': 98, '—': 99, '‘': 100, '’': 101, '“': 102, '”': 103, '•': 104, '′': 105, '￼': 106}\n",
      "idx_to_char = ['\\t', '\\n', ' ', '!', '#', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', '½', 'Å', 'Æ', '×', 'á', 'ä', 'æ', 'è', 'é', 'ë', 'ó', 'ö', 'ü', 'Ο', 'α', 'δ', 'ἶ', '–', '—', '‘', '’', '“', '”', '•', '′', '￼']\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab_size = {}\".format(vocab_size))\n",
    "print(\"embed_dim = {}\".format(embed_dim))\n",
    "print(\"rnn_neurons = {}\".format(rnn_neurons))\n",
    "print(\"batch_size = {}\".format(batch_size))\n",
    "print(\"char_to_idx = {}\".format(char_to_idx))\n",
    "print(\"idx_to_char = {}\".format([thing for thing in idx_to_char]))\n",
    "with open(\"things_for_app.py\",'w') as f:\n",
    "    f.write(\"vocab_size = {}\\n\".format(vocab_size))\n",
    "    f.write(\"embed_dim = {}\\n\".format(embed_dim))\n",
    "    f.write(\"rnn_neurons = {}\\n\".format(rnn_neurons))\n",
    "    f.write(\"batch_size = {}\\n\".format(batch_size))\n",
    "    f.write(\"char_to_idx = {}\\n\".format(char_to_idx))\n",
    "    f.write(\"idx_to_char = {}\".format([thing for thing in idx_to_char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true,y_pred,from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current version of model\n",
    "# def create_model(vocab_size,embed_dim,rnn_neurons,batch_size):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size,embed_dim,batch_input_shape=[batch_size,None]))\n",
    "#     model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,\n",
    "#                  recurrent_initializer='glorot_uniform'))\n",
    "#     model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,\n",
    "#                  recurrent_initializer='glorot_uniform'))\n",
    "#     model.add(Dense(vocab_size))\n",
    "#     model.compile(optimizer='adam',loss=sparse_cat_loss)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing things\n",
    "def create_model(vocab_size,embed_dim,rnn_neurons,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,embed_dim,batch_input_shape=[batch_size,None]))\n",
    "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,\n",
    "                 recurrent_initializer='glorot_uniform'))\n",
    "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,\n",
    "                 recurrent_initializer='glorot_uniform'))\n",
    "#     model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,\n",
    "#                  recurrent_initializer='glorot_uniform'))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile(optimizer='adam',loss=sparse_cat_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size=vocab_size,\n",
    "                    embed_dim=embed_dim,\n",
    "                    rnn_neurons=rnn_neurons,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (256, None, 128)          13696     \n",
      "                                                                 \n",
      " gru_28 (GRU)                (256, None, 512)          986112    \n",
      "                                                                 \n",
      " gru_29 (GRU)                (256, None, 512)          1575936   \n",
      "                                                                 \n",
      " dense_10 (Dense)            (256, None, 107)          54891     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,630,635\n",
      "Trainable params: 2,630,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 250, 107)  <=== (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "  # Predict off some random batch\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "  # Display the dimensions of the predictions\n",
    "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 250, 107), dtype=float32, numpy=\n",
       "array([[[-6.8995738e-05, -2.4763367e-04,  3.4560158e-04, ...,\n",
       "         -3.4601726e-05,  4.9214705e-04,  2.2594002e-05],\n",
       "        [ 1.5224703e-04, -9.4041054e-05,  1.3932060e-03, ...,\n",
       "          1.3681899e-03,  1.5857676e-04, -1.4787825e-04],\n",
       "        [ 1.1248779e-04,  1.9371079e-04,  1.3235305e-03, ...,\n",
       "          1.7012912e-03, -1.9924046e-04,  8.7394757e-05],\n",
       "        ...,\n",
       "        [ 1.7407446e-03,  4.0036514e-03, -3.7174753e-03, ...,\n",
       "          2.3854163e-04, -2.4515169e-04, -2.5167235e-03],\n",
       "        [ 6.5773039e-04,  4.2542722e-03, -3.0742302e-03, ...,\n",
       "          6.3339283e-04,  1.8422923e-04, -3.6075942e-03],\n",
       "        [-2.0459795e-04,  3.9719967e-03, -2.8316854e-03, ...,\n",
       "          4.7716580e-04,  2.1777186e-04, -4.0458245e-03]],\n",
       "\n",
       "       [[ 1.7525585e-04,  5.2358001e-04, -5.8777892e-04, ...,\n",
       "         -1.1985922e-04, -1.4024132e-03,  5.1511941e-04],\n",
       "        [ 4.2646046e-05,  6.0496549e-04, -1.4293035e-03, ...,\n",
       "         -8.0890267e-04, -2.5085281e-03,  9.3802833e-04],\n",
       "        [ 1.0508749e-03, -3.7175210e-04, -2.3076390e-03, ...,\n",
       "         -1.7812696e-03, -2.8120675e-03,  2.2534514e-05],\n",
       "        ...,\n",
       "        [ 2.5386729e-03,  1.0406747e-03, -3.4590578e-03, ...,\n",
       "         -8.8145147e-04, -2.0739737e-03,  1.5871096e-03],\n",
       "        [ 1.9966960e-03,  1.3012849e-03, -2.7037500e-03, ...,\n",
       "         -8.6691417e-04, -2.0108558e-03,  2.3794116e-03],\n",
       "        [ 1.9366679e-03, -3.3330347e-05, -2.4672195e-03, ...,\n",
       "         -1.4482096e-03, -1.7503089e-03,  1.9140269e-03]],\n",
       "\n",
       "       [[-9.5782999e-04,  7.6724536e-05,  5.6613481e-04, ...,\n",
       "          4.4332084e-04,  3.4131168e-05, -7.4778334e-04],\n",
       "        [-1.4963977e-03, -3.8559054e-04,  1.0179672e-03, ...,\n",
       "          6.3201907e-04,  4.7290054e-04, -1.3579400e-03],\n",
       "        [-1.0214858e-03, -4.0952372e-04,  2.1612307e-04, ...,\n",
       "          7.5503974e-04,  1.2023981e-03, -7.3752867e-04],\n",
       "        ...,\n",
       "        [-1.0578524e-03,  1.0166946e-03,  1.1597781e-03, ...,\n",
       "          3.8366732e-03, -7.1363291e-04, -1.2543487e-03],\n",
       "        [-1.0499987e-03,  9.5416541e-04,  1.5179908e-03, ...,\n",
       "          3.9260546e-03, -1.7611257e-04, -1.2040830e-03],\n",
       "        [ 1.5835534e-04,  1.9567733e-04,  3.0080732e-03, ...,\n",
       "          2.6808265e-03,  3.2120082e-04, -4.7414936e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-6.8995738e-05, -2.4763367e-04,  3.4560158e-04, ...,\n",
       "         -3.4601726e-05,  4.9214705e-04,  2.2594002e-05],\n",
       "        [ 3.9571297e-04, -8.8163797e-06, -2.2988050e-04, ...,\n",
       "          7.8762809e-05,  1.1906296e-03,  9.1703609e-04],\n",
       "        [ 7.3794019e-04,  1.4581806e-03,  7.0902403e-05, ...,\n",
       "         -8.1668945e-04,  2.3001153e-03,  8.5799990e-04],\n",
       "        ...,\n",
       "        [-2.1415688e-03,  1.4508028e-03, -1.5396146e-03, ...,\n",
       "         -2.4846513e-03,  2.1921026e-03, -1.1485021e-03],\n",
       "        [-2.3606934e-03,  2.3769280e-03, -1.1140505e-03, ...,\n",
       "         -2.5174457e-03,  3.0421736e-03, -1.5300778e-03],\n",
       "        [-2.2429260e-03,  3.7853946e-03, -1.3688687e-03, ...,\n",
       "         -1.9377636e-03,  3.5327533e-03, -2.1068617e-03]],\n",
       "\n",
       "       [[ 2.0862903e-04,  1.0747428e-03,  7.6573249e-04, ...,\n",
       "         -5.9734826e-04,  1.1387409e-03, -3.0898862e-04],\n",
       "        [-1.2260099e-04,  1.5013432e-03,  2.9102404e-04, ...,\n",
       "         -8.9678739e-04,  1.3098544e-03, -4.3933856e-04],\n",
       "        [ 7.6024188e-04,  3.2930088e-03,  2.8758665e-04, ...,\n",
       "         -9.9777151e-04,  6.7516672e-04, -4.9126323e-04],\n",
       "        ...,\n",
       "        [ 1.9166116e-03,  4.8923874e-03, -7.8991684e-04, ...,\n",
       "          9.3119749e-04, -1.7548414e-03, -1.1982231e-03],\n",
       "        [ 5.9534638e-04,  3.6036936e-03, -9.9394342e-04, ...,\n",
       "         -6.1453320e-06, -1.4791181e-03, -8.3398889e-05],\n",
       "        [ 1.2925244e-04,  3.5556618e-03, -1.9872098e-04, ...,\n",
       "         -1.3104477e-03,  2.9830495e-05, -5.3213851e-05]],\n",
       "\n",
       "       [[ 8.6506782e-04, -1.1160660e-03, -6.2991126e-04, ...,\n",
       "         -6.2589650e-04,  2.3157467e-05, -6.8457436e-04],\n",
       "        [ 1.3046646e-03, -8.7076251e-04, -2.2584386e-04, ...,\n",
       "         -1.4216283e-03,  1.2702564e-03, -1.1096613e-03],\n",
       "        [ 1.4335350e-03,  6.9915585e-04,  1.3969513e-04, ...,\n",
       "         -3.4081779e-04,  9.2638558e-04, -1.7094263e-03],\n",
       "        ...,\n",
       "        [-3.1256070e-04,  2.1892819e-03, -7.4316678e-04, ...,\n",
       "         -1.8379316e-03,  1.0957529e-03, -2.0528215e-03],\n",
       "        [-1.2098369e-04,  3.6767344e-03,  1.2733406e-05, ...,\n",
       "         -6.4850890e-04,  7.2925747e-04, -1.8802762e-03],\n",
       "        [-3.3844623e-04,  4.8014275e-03,  4.4002821e-04, ...,\n",
       "         -1.2126178e-04,  1.1053642e-03, -1.5941733e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(250, 1), dtype=int64, numpy=\n",
       "array([[ 88],\n",
       "       [  8],\n",
       "       [  1],\n",
       "       [ 59],\n",
       "       [ 29],\n",
       "       [ 69],\n",
       "       [ 84],\n",
       "       [ 97],\n",
       "       [102],\n",
       "       [ 69],\n",
       "       [ 71],\n",
       "       [ 55],\n",
       "       [ 21],\n",
       "       [ 82],\n",
       "       [ 40],\n",
       "       [ 68],\n",
       "       [ 62],\n",
       "       [ 46],\n",
       "       [ 38],\n",
       "       [ 34],\n",
       "       [ 97],\n",
       "       [ 49],\n",
       "       [ 34],\n",
       "       [ 59],\n",
       "       [ 44],\n",
       "       [ 34],\n",
       "       [ 62],\n",
       "       [ 12],\n",
       "       [ 48],\n",
       "       [ 39],\n",
       "       [ 21],\n",
       "       [ 57],\n",
       "       [ 79],\n",
       "       [ 82],\n",
       "       [  1],\n",
       "       [ 61],\n",
       "       [ 54],\n",
       "       [ 43],\n",
       "       [ 95],\n",
       "       [ 40],\n",
       "       [ 66],\n",
       "       [ 94],\n",
       "       [ 63],\n",
       "       [ 78],\n",
       "       [ 86],\n",
       "       [ 62],\n",
       "       [ 65],\n",
       "       [102],\n",
       "       [ 45],\n",
       "       [ 95],\n",
       "       [  3],\n",
       "       [ 58],\n",
       "       [ 81],\n",
       "       [ 75],\n",
       "       [ 12],\n",
       "       [  7],\n",
       "       [102],\n",
       "       [  0],\n",
       "       [ 27],\n",
       "       [ 71],\n",
       "       [ 31],\n",
       "       [ 43],\n",
       "       [100],\n",
       "       [105],\n",
       "       [ 51],\n",
       "       [ 55],\n",
       "       [ 34],\n",
       "       [ 55],\n",
       "       [ 86],\n",
       "       [ 39],\n",
       "       [ 28],\n",
       "       [ 25],\n",
       "       [ 15],\n",
       "       [106],\n",
       "       [ 93],\n",
       "       [ 58],\n",
       "       [  9],\n",
       "       [ 21],\n",
       "       [  1],\n",
       "       [ 46],\n",
       "       [ 21],\n",
       "       [ 10],\n",
       "       [102],\n",
       "       [ 36],\n",
       "       [ 60],\n",
       "       [ 11],\n",
       "       [  0],\n",
       "       [ 42],\n",
       "       [ 88],\n",
       "       [  3],\n",
       "       [ 41],\n",
       "       [ 50],\n",
       "       [ 14],\n",
       "       [105],\n",
       "       [ 26],\n",
       "       [ 46],\n",
       "       [ 92],\n",
       "       [ 29],\n",
       "       [ 67],\n",
       "       [ 42],\n",
       "       [ 45],\n",
       "       [103],\n",
       "       [ 67],\n",
       "       [ 73],\n",
       "       [ 61],\n",
       "       [ 90],\n",
       "       [ 84],\n",
       "       [ 66],\n",
       "       [ 67],\n",
       "       [ 66],\n",
       "       [ 45],\n",
       "       [ 95],\n",
       "       [ 10],\n",
       "       [ 78],\n",
       "       [ 43],\n",
       "       [100],\n",
       "       [ 88],\n",
       "       [ 38],\n",
       "       [ 83],\n",
       "       [ 72],\n",
       "       [ 56],\n",
       "       [ 48],\n",
       "       [ 20],\n",
       "       [ 53],\n",
       "       [ 10],\n",
       "       [ 38],\n",
       "       [ 22],\n",
       "       [  3],\n",
       "       [106],\n",
       "       [ 36],\n",
       "       [ 43],\n",
       "       [ 62],\n",
       "       [ 27],\n",
       "       [ 92],\n",
       "       [ 72],\n",
       "       [ 95],\n",
       "       [ 62],\n",
       "       [101],\n",
       "       [ 85],\n",
       "       [ 13],\n",
       "       [ 40],\n",
       "       [ 65],\n",
       "       [ 47],\n",
       "       [ 43],\n",
       "       [ 33],\n",
       "       [ 60],\n",
       "       [ 61],\n",
       "       [  8],\n",
       "       [ 50],\n",
       "       [ 22],\n",
       "       [ 18],\n",
       "       [ 85],\n",
       "       [ 98],\n",
       "       [ 43],\n",
       "       [ 91],\n",
       "       [ 62],\n",
       "       [ 32],\n",
       "       [ 10],\n",
       "       [ 42],\n",
       "       [ 82],\n",
       "       [  4],\n",
       "       [ 42],\n",
       "       [ 18],\n",
       "       [ 20],\n",
       "       [  5],\n",
       "       [ 46],\n",
       "       [ 86],\n",
       "       [ 67],\n",
       "       [ 49],\n",
       "       [ 99],\n",
       "       [ 90],\n",
       "       [ 72],\n",
       "       [ 13],\n",
       "       [ 64],\n",
       "       [ 27],\n",
       "       [ 19],\n",
       "       [102],\n",
       "       [ 98],\n",
       "       [ 20],\n",
       "       [ 75],\n",
       "       [ 38],\n",
       "       [ 52],\n",
       "       [ 17],\n",
       "       [  4],\n",
       "       [ 61],\n",
       "       [  9],\n",
       "       [ 24],\n",
       "       [ 73],\n",
       "       [ 55],\n",
       "       [ 12],\n",
       "       [ 89],\n",
       "       [ 10],\n",
       "       [ 85],\n",
       "       [  2],\n",
       "       [ 55],\n",
       "       [ 31],\n",
       "       [ 17],\n",
       "       [ 45],\n",
       "       [ 72],\n",
       "       [ 26],\n",
       "       [ 20],\n",
       "       [ 26],\n",
       "       [ 25],\n",
       "       [ 42],\n",
       "       [  9],\n",
       "       [ 88],\n",
       "       [ 31],\n",
       "       [ 32],\n",
       "       [ 62],\n",
       "       [ 41],\n",
       "       [ 90],\n",
       "       [ 54],\n",
       "       [ 82],\n",
       "       [ 44],\n",
       "       [ 37],\n",
       "       [ 49],\n",
       "       [106],\n",
       "       [ 31],\n",
       "       [ 19],\n",
       "       [ 21],\n",
       "       [ 35],\n",
       "       [ 57],\n",
       "       [ 48],\n",
       "       [ 29],\n",
       "       [ 71],\n",
       "       [  3],\n",
       "       [ 63],\n",
       "       [ 78],\n",
       "       [  0],\n",
       "       [ 98],\n",
       "       [102],\n",
       "       [ 47],\n",
       "       [100],\n",
       "       [ 35],\n",
       "       [ 46],\n",
       "       [ 26],\n",
       "       [ 24],\n",
       "       [100],\n",
       "       [ 54],\n",
       "       [ 63],\n",
       "       [ 74],\n",
       "       [ 25],\n",
       "       [105],\n",
       "       [ 50],\n",
       "       [ 95],\n",
       "       [ 18],\n",
       "       [ 15],\n",
       "       [ 76],\n",
       "       [  6],\n",
       "       [104]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88,   8,   1,  59,  29,  69,  84,  97, 102,  69,  71,  55,  21,\n",
       "        82,  40,  68,  62,  46,  38,  34,  97,  49,  34,  59,  44,  34,\n",
       "        62,  12,  48,  39,  21,  57,  79,  82,   1,  61,  54,  43,  95,\n",
       "        40,  66,  94,  63,  78,  86,  62,  65, 102,  45,  95,   3,  58,\n",
       "        81,  75,  12,   7, 102,   0,  27,  71,  31,  43, 100, 105,  51,\n",
       "        55,  34,  55,  86,  39,  28,  25,  15, 106,  93,  58,   9,  21,\n",
       "         1,  46,  21,  10, 102,  36,  60,  11,   0,  42,  88,   3,  41,\n",
       "        50,  14, 105,  26,  46,  92,  29,  67,  42,  45, 103,  67,  73,\n",
       "        61,  90,  84,  66,  67,  66,  45,  95,  10,  78,  43, 100,  88,\n",
       "        38,  83,  72,  56,  48,  20,  53,  10,  38,  22,   3, 106,  36,\n",
       "        43,  62,  27,  92,  72,  95,  62, 101,  85,  13,  40,  65,  47,\n",
       "        43,  33,  60,  61,   8,  50,  22,  18,  85,  98,  43,  91,  62,\n",
       "        32,  10,  42,  82,   4,  42,  18,  20,   5,  46,  86,  67,  49,\n",
       "        99,  90,  72,  13,  64,  27,  19, 102,  98,  20,  75,  38,  52,\n",
       "        17,   4,  61,   9,  24,  73,  55,  12,  89,  10,  85,   2,  55,\n",
       "        31,  17,  45,  72,  26,  20,  26,  25,  42,   9,  88,  31,  32,\n",
       "        62,  41,  90,  54,  82,  44,  37,  49, 106,  31,  19,  21,  35,\n",
       "        57,  48,  29,  71,   3,  63,  78,   0,  98, 102,  47, 100,  35,\n",
       "        46,  26,  24, 100,  54,  63,  74,  25, 105,  50,  95,  18,  15,\n",
       "        76,   6, 104])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the input seq: \n",
      "\n",
      "n; and she could not but feel that he meant some vague, indefinable harm to Alfred. She did not like the Thibetans, either, and thought it very peculiar that Surama was able to talk with them. Alfred would not tell her who or what Surama was, but had\n",
      "\n",
      "\n",
      "Next Char Predictions: \n",
      "\n",
      "è(\n",
      "fDp×ἶ“prb8ÅOoiUMIἶXIfSIi.WN8dzÅ\n",
      "haRαOmΟjyäil“Tα!e½v.'“\tBrFR‘′ZbIbäNC?2￼üe)8\n",
      "U8,“Kg-\tQè!PY1′AUöDnQT”nthë×mnmTα,yR‘èMÆscW7],M9!￼KRiBösαi’á0OlVRHgh(Y95á–RóiG,QÅ#Q57$UänX—ës0kB6“–7vM[4#h);tb.é,á bF4TsA7A?Q)èFGiPëaÅSLX￼F68JdWDr!jy\t–“V‘JUA;‘aju?′Yα52w&•\n"
     ]
    }
   ],
   "source": [
    "print(\"Given the input seq: \\n\")\n",
    "print(\"\".join(idx_to_char[input_example_batch[0]]))\n",
    "print('\\n')\n",
    "print(\"Next Char Predictions: \\n\")\n",
    "print(\"\".join(idx_to_char[sampled_indices ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 30s 454ms/step - loss: 3.5691\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 90s 2s/step - loss: 2.8815\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 115s 2s/step - loss: 2.4782\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 122s 2s/step - loss: 2.2868\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 135s 3s/step - loss: 2.1223\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 145s 3s/step - loss: 1.9809\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 137s 3s/step - loss: 1.8628\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 137s 3s/step - loss: 1.7675\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 146s 3s/step - loss: 1.6897\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 141s 3s/step - loss: 1.6271\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 137s 3s/step - loss: 1.5753\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 144s 3s/step - loss: 1.5333\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 149s 3s/step - loss: 1.4971\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 161s 3s/step - loss: 1.4663\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 154s 3s/step - loss: 1.4408\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 150s 3s/step - loss: 1.4176\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 150s 3s/step - loss: 1.3980\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 164s 3s/step - loss: 1.3808\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 144s 3s/step - loss: 1.3649\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 152s 3s/step - loss: 1.3506\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 137s 3s/step - loss: 1.3374\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 161s 3s/step - loss: 1.3252\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 148s 3s/step - loss: 1.3140\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 143s 3s/step - loss: 1.3037\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 147s 3s/step - loss: 1.2936\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 149s 3s/step - loss: 1.2849\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 145s 3s/step - loss: 1.2758\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 146s 3s/step - loss: 1.2678\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 150s 3s/step - loss: 1.2588\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 151s 3s/step - loss: 1.2511\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 159s 3s/step - loss: 1.2441\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 142s 3s/step - loss: 1.2363\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 150s 3s/step - loss: 1.2287\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 1.2229\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 146s 3s/step - loss: 1.2159\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 159s 3s/step - loss: 1.2090\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 152s 3s/step - loss: 1.2022\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 142s 3s/step - loss: 1.1961\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 165s 3s/step - loss: 1.1899\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 153s 3s/step - loss: 1.1834\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 155s 3s/step - loss: 1.1769\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 1.1713\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 1.1656\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 153s 3s/step - loss: 1.1587\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 1.1532\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 153s 3s/step - loss: 1.1480\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 186s 4s/step - loss: 1.1420\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 151s 3s/step - loss: 1.1360\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 170s 3s/step - loss: 1.1306\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 148s 3s/step - loss: 1.1246\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 148s 3s/step - loss: 1.1183\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 153s 3s/step - loss: 1.1127\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 148s 3s/step - loss: 1.1072\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 161s 3s/step - loss: 1.1013\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 153s 3s/step - loss: 1.0955\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 1.0901\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 131s 2s/step - loss: 1.0843\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 143s 3s/step - loss: 1.0789\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 1.0739\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 167s 3s/step - loss: 1.0681\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 162s 3s/step - loss: 1.0619\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 1.0569\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 162s 3s/step - loss: 1.0518\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 161s 3s/step - loss: 1.0456\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 167s 3s/step - loss: 1.0401\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 144s 3s/step - loss: 1.0350\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 182s 3s/step - loss: 1.0290\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 136s 3s/step - loss: 1.0237\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 1.0181\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 150s 3s/step - loss: 1.0137\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 1.0085\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 148s 3s/step - loss: 1.0029\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 142s 3s/step - loss: 0.9972\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 165s 3s/step - loss: 0.9918\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 140s 3s/step - loss: 0.9867\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 154s 3s/step - loss: 0.9822\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 149s 3s/step - loss: 0.9765\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 157s 3s/step - loss: 0.9723\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 157s 3s/step - loss: 0.9671\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 159s 3s/step - loss: 0.9619\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 145s 3s/step - loss: 0.9575\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 147s 3s/step - loss: 0.9520\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 142s 3s/step - loss: 0.9478\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 149s 3s/step - loss: 0.9433\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 149s 3s/step - loss: 0.9390\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 153s 3s/step - loss: 0.9338\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 133s 3s/step - loss: 0.9306\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 148s 3s/step - loss: 0.9253\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 163s 3s/step - loss: 0.9194\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 160s 3s/step - loss: 0.9165\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 127s 2s/step - loss: 0.9123\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 152s 3s/step - loss: 0.9065\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 151s 3s/step - loss: 0.9025\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 157s 3s/step - loss: 0.8993\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 142s 3s/step - loss: 0.8945\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 138s 3s/step - loss: 0.8904\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 154s 3s/step - loss: 0.8873\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 154s 3s/step - loss: 0.8835\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 138s 3s/step - loss: 0.8804\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 146s 3s/step - loss: 0.8748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0d03acf10>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "###model.save('shakecraft_gen2.h5')\n",
    "model.save('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model(vocab_size,embed_dim,rnn_neurons,batch_size=1)\n",
    "# model2.load_weights('shakecraft_gen2.h5')\n",
    "model2.load_weights('test.h5')\n",
    "model2.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (1, None, 128)            13696     \n",
      "                                                                 \n",
      " gru_30 (GRU)                (1, None, 512)            986112    \n",
      "                                                                 \n",
      " gru_31 (GRU)                (1, None, 512)            1575936   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (1, None, 107)            54891     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,630,635\n",
      "Trainable params: 2,630,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters.\n",
    "  '''\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = gen_size\n",
    "\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char_to_idx[s] for s in start_seed]\n",
    "\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to affect probability of next characters.\n",
    "  # Higher temperature ==> lesss surprising/ more expected\n",
    "  # Lower temperature ==> more surprising / less expected\n",
    " \n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "\n",
    "  for i in range(num_generate):\n",
    "\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # Use a categorircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(idx_to_char[predicted_id])\n",
    "\n",
    "  return (start_seed + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat over there. Another poly ground built himself upon it, but stopp'd\n",
      "would prance of meet and ne'er liveller.\n",
      "Hasty words, and what she shall shives books;\n",
      "And must of your power--with twelve spine.\n",
      "\n",
      "SILVIA\n",
      "\n",
      "Sir on your arm?\n",
      "\n",
      "EMILIA\n",
      "\n",
      "O, faithful 'aive; further, or pity her with women,\n",
      "Where 't still already stand\n",
      "Or else is too great a man:\n",
      "A thankffir throat in the can pass,\n",
      "But yet no other from your nequent contend\n",
      "As truth, he calls me not enter a self, I get concerned up to get them spring: but I must put most of their life to babe!”\n",
      "     Bruce—seeing, he had never continued to congratu the Court of Azathoth and It night. I myself only a white wall could bring the fish-during land.\n",
      "     The youth had counting to recall the evil country home, nor any special moul. The doctor and the faithful chanthis came over Inss that late in all the minutes of the lower Stampers’ blood-tried to examine the fact that whippoorward thereof could see away with this accursed structure, I seemed completion design. When I reveleed more and more too dark forehead. In a moment the mysterious sickless planet by disdosition, he began to write in awful moment is strangely and disintegrate and forbidden record and bubble, and separate the a night the whirmphrower will be let blood,\n",
      "No couple and each scream had drinks us;\n",
      "But in whom your shadows golden wither'd turn,\n",
      "Forward toucheadd the awful time,\n",
      "And waspish it. But this same being is lembered heads at once, so that the code of the significance of an extreme and unfortunate whispers and seating anomality of a revolver; but sagn half doubt up in front of uncouth, hideous chinshinks were planned,\n",
      "Bisnd in justice, in the morning, in her cousina that he had always revealed that my chose loungerons. I am the just alive, suddenly to the grey joined, almost change of mystic territting voice, until the hy saw that the dense genivation of the ancient Ward had nof myself too column; a trifle pretty trumpey when Lrub where the slimy stairs. It was harmless\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model2,\"cat over there.\",gen_size=2000,temp=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
